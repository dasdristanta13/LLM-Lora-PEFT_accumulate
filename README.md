# LLM-Lora-PEFT_accumulate
This constitutes available resources to learn LORA/QLORA for LLM.I myself is learning and looking for resources, if anyone wants to contribute, please feel free to do so:
- [HF-BitsandBytes-Integration](https://huggingface.co/blog/hf-bitsandbytes-integration)
- [LLM.int8() and Emergent Features](https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/)
- [Tensorfloat-32-precision-format](https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/)
- [Paper- LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://arxiv.org/pdf/2208.07339.pdf)
- [YT Video- Boost Fine-Tuning Performance of LLM: Optimal Architecture w/ PEFT LoRA Adapter-Tuning on Your GPU](https://youtu.be/A-a-l_sFtYM)
- [Finetune-opt-bnb-peft.ipynb](https://colab.research.google.com/drive/1jCkpikz0J2o20FBQmYmAGdiKmJGOMo-o?usp=sharing#scrollTo=S65GcxNGA9kz)
- [YT Video- How to finetune your own Alpaca 7B](https://youtu.be/LSoqyynKU9E)
- [ðŸ¤— PEFT: Parameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource Hardware](https://huggingface.co/blog/peft)
- [TLoen Github Repo](https://github.com/tloen/alpaca-lora)
