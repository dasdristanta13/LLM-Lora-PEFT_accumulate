# LLM-Lora-PEFT_accumulate
This constitutes available resources to learn LORA/QLORA for LLM.I am learning myself and looking for resources, if anyone wants to contribute, please feel free to do so:
- üîç Web Sites
  - [HF-BitsandBytes-Integration](https://huggingface.co/blog/hf-bitsandbytes-integration)
  - [ü§ó PEFT: Parameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource Hardware](https://huggingface.co/blog/peft)
  - [LLM.int8() and Emergent Features](https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/)
  - [Tensorfloat-32-precision-format](https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/)
- Youtube Videos
  - [Boost Fine-Tuning Performance of LLM: Optimal Architecture w/ PEFT LoRA Adapter-Tuning on Your GPU](https://youtu.be/A-a-l_sFtYM)
  - [How to finetune your own Alpaca 7B](https://youtu.be/LSoqyynKU9E)
- Papers
  - [LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://arxiv.org/pdf/2208.07339.pdf)
- Github Repository
  - [TLoen Github Repo](https://github.com/tloen/alpaca-lora)


